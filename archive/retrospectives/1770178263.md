# Session Retrospective: /task #119 -- MVP Test Suite Expansion

**Entry ID:** 1770178263
**Date:** 2026-02-03
**Skill:** /task
**Issue:** #119 (closed), PR #130 (merged)
**Outcome:** 102 new tests across 6 files, 302 total tests, 766 evergreen passed, 0 failures

---

## 1. User Orchestration

**Effective patterns:**
- User answered 5 alignment questions concisely via the multiple-choice format. Responses were direct and decisive, particularly "SBS-Test only for Phase 1" and "all evergreen."
- User's plan rejections were precise and actionable: first rejection identified insufficient data verification ("make more expansive use of manual review via agents"), second rejection clarified scope precisely ("drop all of the tests not directly mentioned in the MVP, make at least 100 tests").
- The user established MVP.md as the sole source of truth for test scope, which was a clarification that prevented substantial wasted work.

**Areas for improvement:**
- User had to explicitly state "we no longer support verso pages for MVP" -- this information exists in CLAUDE.md under Known Limitations ("All Verso page types have been removed from active surfaces"). The agent should have known this without asking.
- The "at least 100 tests" requirement came during the second plan rejection. Earlier surfacing of quantitative expectations during alignment could have avoided the second plan revision.

**Observation for L2:** The user's correction pattern suggests they expected Claude to already know project-specific constraints from CLAUDE.md. Pre-reading and synthesizing Known Limitations before alignment questions would prevent this class of error.

## 2. Claude Alignment

**What went wrong:**
- **Verso pages inclusion (plan 1 and 2):** Tests were proposed for `paper_verso` and `blueprint_verso` pages despite CLAUDE.md explicitly listing them as removed from active surfaces. This was a failure to internalize the Known Limitations section before planning.
- **GOALS.md scope leak (plan 2):** The first approved plan included tests for items from GOALS.md that were not in MVP.md. The user had to explicitly redirect: "drop all of the tests not directly mentioned in the MVP." This shows the agent conflated the product vision (GOALS.md) with the current deliverable specification (MVP.md).
- **Colon-to-hyphen ID mapping:** Node IDs use colons in Lean (`thm:main`) but are normalized to hyphens in HTML (`thm-main`). This was discovered at runtime when 13 tests failed. The information exists in sbs-developer.md under "ID Normalization" but was not applied during test writing.

**What went right:**
- After the two rejections, the third plan was tightly mapped to MVP.md criteria with a 1:1 correspondence.
- Data exploration agents in Wave 0 of the approved plan successfully gathered manifest.json structure, HTML patterns, and CSS variable names. This prevented many more failures than the 13 that occurred.
- The fix wave (Wave 4) efficiently resolved all 13 failures in a single pass.

**Assumption errors:**
- Assumed dashboard stats in manifest.json would directly match HTML display values. In reality, Runway computes aggregated statistics (e.g., "Completion" percentage) that don't appear as raw fields in manifest.json. Tests had to be rewritten to check computed values rather than raw data.
- Assumed all MVP criteria required code-level testing. Some criteria (like "build completes without error") were already covered by existing infrastructure.

## 3. System Design

**Tooling that worked well:**
- Parallel agent waves (3 waves of 2 agents each) provided clean parallelism for independent test files. No file collisions occurred.
- The pytest evergreen tier marker system worked correctly -- all new tests were properly marked `@pytest.mark.evergreen` and integrated into the existing gate mechanism.
- The `conftest.py` fixture infrastructure (manifest data, site path, parsed HTML) was reusable and well-structured for the new tests.

**Tooling friction:**
- No friction from MCP tools in this session. The primary friction was conceptual (understanding data transformations between Lean/manifest/HTML) rather than tooling-related.
- The `helpers.py` module in the MVP test directory needed 5 new helper functions (`get_status_color_hex`, `parse_manifest_nodes`, `get_css_variables`, `normalize_node_id`, `get_html_status_dots`). These were straightforward additions but suggest the helper module was underscoped for the full MVP test surface.

**Missing automation:**
- No automated way to preview test coverage mapping against MVP.md criteria. A tool that could compare test names/docstrings against MVP.md bullet points would have caught the scope drift earlier.

## 4. Plan Execution

**Plan adherence:** The approved (third) plan was executed faithfully. All 6 test files were created, all mapped to the expected MVP criteria.

**Mid-flight adjustments:**
- Wave 4 (fix pass) was not in the original plan but was anticipated as a contingency. It addressed exactly the kind of failures the data exploration wave was designed to prevent but couldn't fully eliminate.
- The 13 failures broke down as: 5 colon/hyphen ID issues, 4 dashboard aggregation mismatches, 2 Verso page assertions (pages don't exist), 2 threshold sensitivity issues.

**What was underestimated:**
- The gap between manifest.json data model and rendered HTML. The dashboard doesn't just display raw manifest data -- it computes derived statistics. This required understanding Runway's rendering logic, not just the manifest schema.
- The number of distinct HTML patterns to test. Each page type (dashboard, chapter, dep_graph, paper_tex) has its own DOM structure with different class names, element hierarchies, and data presentation patterns.

## 5. Meta-Observations

**Archive system observations:**
- 29 entries accumulated in this epoch, reflecting a substantial task. The entry count tracks well with the complexity of the work.
- The PR workflow (create at plan approval, merge at finalization) worked smoothly. PR #130 was created, used for branch tracking, and merged without issues.

**Documentation quality:**
- CLAUDE.md's Known Limitations section proved critical but was not sufficiently consulted. Future agents should have a "pre-flight checklist" that includes reading Known Limitations before any planning work.
- MVP.md was well-structured for mapping to test criteria. Each bullet point was specific enough to derive test assertions from.

**Test infrastructure observations:**
- The evergreen test tier now has 766 tests, up from ~200. This is a significant increase in regression coverage.
- All new tests use threshold-based assertions (e.g., "at least 5 nodes" rather than "exactly 32 nodes"). This makes them resilient to project growth -- a pattern that should be standard for MVP tests.
- The test organization (one file per MVP category) creates clear ownership and makes it easy to trace failures back to specific MVP criteria.

**Pattern for L2 synthesis:**
- Two plan rejections before approval is a signal that alignment questions should probe scope constraints more aggressively. Specifically: "What is the authoritative source for test scope?" should be asked early.
- Data exploration before test writing (Wave 0 in the plan) is a pattern worth formalizing. The 13 failures that remained were all in areas the exploration agents didn't cover deeply enough (dashboard rendering logic, ID normalization in HTML).

---

## Key Findings (for archive entry notes)

1. Two plan rejections traced to insufficient internalization of MVP.md as sole source and CLAUDE.md Known Limitations (Verso pages removed)
2. 13/102 test failures on first run, all fixed in single pass -- primarily colon-to-hyphen ID normalization and dashboard aggregation mismatches
3. Parallel agent waves (3x2) worked cleanly for independent test files with zero file collisions
4. Threshold-based assertions pattern adopted across all MVP tests for resilience to project growth
5. Test count: 200 -> 302 total, 766 evergreen passing, 0 failures, 0 regressions
